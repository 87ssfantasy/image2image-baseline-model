'''
transfer dataset from multiple npz (generated by Enhao's code) to a single pickle file
the picke file is a list of dict, each dict includes 'input' and 'output'
'''
import argparse
import numpy as np
import os
import sys
import pdb
import pickle
from glob import glob

parser = argparse.ArgumentParser(description='')
parser.add_argument('--dataset_dir', dest='dataset_dir', default='data/train', help='dir for .npz file generated by previous data preparation code')
parser.add_argument('--data_type', dest='data_type', default='npz', help='only support loading npz now')
parser.add_argument('--dataset_dst_path', dest='dataset_dst_path', default='data/train.pickle')
args = parser.parse_args()

dataset_dir = args.dataset_dir
if dataset_dir[-1] == '/':
    dataset_dir = dataset_dir[:-1]
sample_files = glob('{}/*.{}'.format(dataset_dir, args.data_type))
print('total '+str(len(sample_files))+' files')
sample_files = sample_files[:100]

# sort
n = [int(i) for i in map(lambda x: x.split('/')[-1].split('.'+args.data_type)[0], sample_files)]
sample_files = [x for (y, x) in sorted(zip(n, sample_files))]

# load all files and make pickle
sample_list = []
for path in sample_files:
    sample = np.load(path)
    sample_new = {'input':sample['input'], 'target':sample['output']}
    sample_list.append(sample_new)
print('finished loading all data')
# pdb.set_trace()

with open(args.dataset_dst_path, 'wb') as handle:
    pickle.dump(sample_list, handle, protocol=2)
print('saved data to pickle file')
